{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AP3 - Pattern Recognition\n",
    "Implementation of a decision tree for classifying a database with categorical attributes.\n",
    "\n",
    "> Name: Jonas Carvalho Fortes\n",
    "\n",
    "> Mat: 494513"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (876, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "dataset = loadmat('data/Dataset.mat')\n",
    "dataset = pd.DataFrame(dataset['Dataset'])\n",
    "print(f'Dataset shape: {dataset.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>876 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3\n",
       "0    1  0  0  1\n",
       "1    0  0  0  1\n",
       "2    0  0  0  0\n",
       "3    0  0  0  0\n",
       "4    0  0  0  1\n",
       "..  .. .. .. ..\n",
       "871  1  1  0  1\n",
       "872  1  1  0  1\n",
       "873  1  1  0  1\n",
       "874  1  0  1  1\n",
       "875  1  0  0  1\n",
       "\n",
       "[876 rows x 4 columns]"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dataset:')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np \n",
    "\n",
    "# Menor valor de entropia possível que não seja zero (usado para evitar divisão por zero)\n",
    "EPSILON = np.finfo('float32').eps\n",
    "\n",
    "# Calcula a entropia de um conjunto de rótulos\n",
    "def entropy(labels):\n",
    "    \"\"\"\n",
    "    Calcula a entropia de um conjunto de rótulos.\n",
    "    \n",
    "    Args:\n",
    "        labels (np.ndarray): Array contendo os rótulos dos dados (0 ou 1).\n",
    "    \n",
    "    Returns:\n",
    "        float: Entropia do conjunto de rótulos.\n",
    "    \"\"\"\n",
    "    # Calcula a frequência de cada classe\n",
    "    values, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Calcula a probabilidade de cada classe\n",
    "    probabilities = counts / len(labels)\n",
    "    \n",
    "    # Calcula a entropia usando a fórmula da entropia\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities + EPSILON))\n",
    "    \n",
    "    return entropy_value\n",
    "\n",
    "# Calcula o ganho de informação de uma divisão dos dados\n",
    "def information_gain(original_labels, left_labels, right_labels):\n",
    "    \"\"\"\n",
    "    Calcula o ganho de informação de uma divisão dos dados.\n",
    "    \n",
    "    Args:\n",
    "        original_labels (np.ndarray): Rótulos antes da divisão.\n",
    "        left_labels (np.ndarray): Rótulos do subconjunto à esquerda.\n",
    "        right_labels (np.ndarray): Rótulos do subconjunto à direita.\n",
    "    \n",
    "    Returns:\n",
    "        float: Ganho de informação da divisão.\n",
    "    \"\"\"\n",
    "    # Entropia antes da divisão\n",
    "    original_entropy = entropy(original_labels)\n",
    "    \n",
    "    # Calcula a entropia ponderada após a divisão\n",
    "    total_len = len(original_labels)\n",
    "    left_prob = len(left_labels) / total_len\n",
    "    right_prob = len(right_labels) / total_len\n",
    "    \n",
    "    # Entropia ponderada após a divisão\n",
    "    weighted_entropy = (left_prob * entropy(left_labels)) + (right_prob * entropy(right_labels))\n",
    "    \n",
    "    # Ganho de informação\n",
    "    gain = original_entropy - weighted_entropy\n",
    "    \n",
    "    return gain\n",
    "\n",
    "# Obtém a classe majoritária de um conjunto de rótulos\n",
    "def get_majority_class(labels):\n",
    "    \"\"\"\n",
    "    Obtém a classe majoritária de um conjunto de rótulos.\n",
    "    \n",
    "    Args:\n",
    "        labels (np.ndarray): Array contendo os rótulos dos dados (0 ou 1).\n",
    "    \n",
    "    Returns:\n",
    "        int: A classe majoritária.\n",
    "    \"\"\"\n",
    "    # Conta a frequência de cada classe\n",
    "    values, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Retorna a classe com a maior frequência\n",
    "    majority_class = values[np.argmax(counts)]\n",
    "    \n",
    "    return majority_class\n",
    "\n",
    "# Função para realizar K-fold cross-validation com a árvore de decisão\n",
    "def k_fold_cross_validation(X, y, k_folds, decision_tree_class):\n",
    "    \"\"\"\n",
    "    Realiza K-fold cross-validation com a árvore de decisão \n",
    "    e retorna as métricas médias de cada fold.\n",
    "    \n",
    "    Args: \n",
    "        X (np.ndarray): Dados de entrada.\n",
    "        y (np.ndarray): Rótulos dos dados.\n",
    "        k_folds (int): Número de folds.\n",
    "        decision_tree_class (class): Classe da árvore de decisão.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dicionário contendo as métricas médias de cada fold.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Dividir os dados em K folds\n",
    "    fold_size = len(y) // k_folds\n",
    "    folds_X = [X[i * fold_size:(i + 1) * fold_size] for i in range(k_folds)]\n",
    "    folds_y = [y[i * fold_size:(i + 1) * fold_size] for i in range(k_folds)]\n",
    "\n",
    "    # Listas para armazenar as métricas de cada fold\n",
    "    accuracies = []\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    "    precisions = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Iterar sobre cada fold\n",
    "    for i in range(k_folds):\n",
    "        \n",
    "        # Separar os dados em treino e teste\n",
    "        X_test = folds_X[i]\n",
    "        y_test = folds_y[i]\n",
    "        \n",
    "        # Concatenar os outros folds para treino\n",
    "        X_train = np.concatenate([folds_X[j] for j in range(k_folds) if j != i], axis=0)\n",
    "        y_train = np.concatenate([folds_y[j] for j in range(k_folds) if j != i], axis=0)\n",
    "\n",
    "        # Criar uma nova instância da árvore de decisão e treinar com os dados de treinamento\n",
    "        tree = decision_tree_class()\n",
    "        tree.fit(X_train, y_train) # Treinar a árvore\n",
    "\n",
    "        # Fazer predições nos dados de teste\n",
    "        y_pred = [tree.predict(X_test[j]) for j in range(len(X_test))]\n",
    "\n",
    "        # Calcular a matriz de confusão\n",
    "        \"\"\"\n",
    "        tn: verdadeiros negativos, \n",
    "        fp: falsos positivos, \n",
    "        fn: falsos negativos, \n",
    "        tp: verdadeiros positivos\n",
    "        \"\"\"\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        # Calcular as métricas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        sensitivity = recall_score(y_test, y_pred)  # Sensibilidade é o mesmo que recall\n",
    "        specificity = tn / (tn + fp)  # Especificidade: proporção de verdadeiros negativos\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Armazenar as métricas\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        sensitivities.append(sensitivity)\n",
    "        specificities.append(specificity)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Calcular a média das métricas\n",
    "    mean_accuracy = float(np.mean(accuracies).round(2))\n",
    "    mean_precision = float(np.mean(precisions).round(2))\n",
    "    mean_sensitivity = float(np.mean(sensitivities).round(2))\n",
    "    mean_specificity = float(np.mean(specificities).round(2))\n",
    "    mean_f1_score = float(np.mean(f1_scores).round(2))\n",
    "\n",
    "    return {\n",
    "        'accuracy': f'{mean_accuracy*100}%',\n",
    "        'precision': f'{mean_precision*100}%',\n",
    "        'sensitivity': f'{mean_sensitivity*100}%',\n",
    "        'specificity': f'{mean_specificity*100}%',\n",
    "        'f1_score': f'{mean_f1_score*100}%'\n",
    "    }\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Nó genérico de uma árvore de decisão.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, data=None, feature_idx=None, \n",
    "                 feature_name=None, criterion_value=None,\n",
    "                 _result=None, class_name=None):\n",
    "        \n",
    "        self.left: Node = left\n",
    "        self.right: Node = right\n",
    "\n",
    "        self.data = data\n",
    "        self.feature_idx: int = feature_idx\n",
    "        self.feature_name: str = feature_name\n",
    "        self.criterion_value: float = criterion_value\n",
    "        self.n_sample: int = len(data) if data is not None else 0\n",
    "        self._result = _result\n",
    "        self.class_name: str = class_name\n",
    "\n",
    "    def predict(self, x):\n",
    "        if x[self.feature_idx] == 0:\n",
    "            return self.left.predict(x)\n",
    "        else:  # Aqui x[self.feature_idx] deve ser 1\n",
    "            return self.right.predict(x)\n",
    "\n",
    "\n",
    "class LeafNode(Node):\n",
    "    \n",
    "    \"\"\"\n",
    "    Nó folha de uma árvore de decisão. \n",
    "    Este nó não possui filhos e é usado para parar a recursão.\n",
    "    É uma extensão da classe Node.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, criterion_value, _result, class_name):\n",
    "        super().__init__(None, None, data=data, \n",
    "                         criterion_value=criterion_value, \n",
    "                         _result=_result, class_name=class_name)\n",
    "\n",
    "    def predict(self, X=None):\n",
    "        return self._result\n",
    "    \n",
    "    \n",
    "class DecisionTree: \n",
    "    \"\"\"\n",
    "    Implementação de uma árvore de decisão binária.\n",
    "    Usando a entropia como critério de divisão.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_names=['Empregado', 'Devedor', 'Salário>5SM'], class_names=['não', 'sim']):\n",
    "        self.root_node: Node = Node(None, None)\n",
    "        self.feature_names = feature_names\n",
    "        self.class_names = class_names\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "         # Cria um array de dados que inclui atributos e rótulos\n",
    "        data = np.hstack((X, y.reshape(-1, 1)))\n",
    "        # Chama o método grow para construir toda a árvore a partir da raiz\n",
    "        self.root_node = self._grow(data)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.root_node.predict(X)\n",
    "\n",
    "    def _split_data(self, feature_data, labels):\n",
    "        \"\"\"\n",
    "        Divide os dados com base nos valores categóricos (0 ou 1) do atributo.\n",
    "        \"\"\"\n",
    "        # Dividindo à esquerda (valor 0)\n",
    "        left_indices = feature_data == 0\n",
    "        left_labels = labels[left_indices]\n",
    "\n",
    "        # Dividindo à direita (valor 1)\n",
    "        right_indices = feature_data == 1\n",
    "        right_labels = labels[right_indices]\n",
    "\n",
    "        return (left_labels, right_labels)\n",
    "    \n",
    "    def _best_feature(self, data, feature_idxs):\n",
    "        \"\"\"\n",
    "        Identifica o melhor atributo binário para dividir os dados.\n",
    "        \"\"\"\n",
    "        # Inicializa as variáveis de controle\n",
    "        max_gain = -np.inf\n",
    "        selected_feature = None\n",
    "\n",
    "        # Itera sobre os índices dos atributos\n",
    "        for feature_idx in feature_idxs:\n",
    "            # Obtém os dados do atributo\n",
    "            feature_data = data[:, feature_idx]\n",
    "            labels = data[:, -1]\n",
    "            \n",
    "            # Avalia o ganho de informação para o atributo atual\n",
    "            left_labels, right_labels = self._split_data(feature_data, labels)\n",
    "            \n",
    "            # Calcula o ganho de informação\n",
    "            gain = information_gain(labels, left_labels, right_labels)\n",
    "\n",
    "            # Atualiza o atributo selecionado se o ganho for maior\n",
    "            if gain > max_gain:\n",
    "                max_gain = gain\n",
    "                selected_feature = feature_idx\n",
    "                \n",
    "        # Retorna o índice do atributo selecionado e o ganho de informação\n",
    "        return selected_feature, max_gain\n",
    "    \n",
    "    def _grow(self, data, used_features=None):\n",
    "        \"\"\"\n",
    "        Constrói recursivamente a árvore de decisão.\n",
    "        \"\"\"\n",
    "        if used_features is None:\n",
    "            used_features = set()\n",
    "\n",
    "        # Define o uso das funções de entropia e classe majoritária\n",
    "        compute_criterion_value = entropy\n",
    "        get_result = get_majority_class\n",
    "\n",
    "        # Obtém os rótulos dos dados\n",
    "        y = data[:, -1]\n",
    "        \n",
    "        # Calcula a entropia dos dados\n",
    "        criterion_value = compute_criterion_value(y)\n",
    "        \n",
    "        # Obtém a classe majoritária dos dados\n",
    "        class_result = get_result(y)\n",
    "        \n",
    "        # Define o nome da classe (usado para visualização)\n",
    "        class_name = self.class_names[class_result] if self.class_names else f\"{class_result:.4f}\"\n",
    "\n",
    "        # Critério de parada: parar se a entropia for zero (dados puros) ou se todos os atributos já tiverem sido usados\n",
    "        if criterion_value < EPSILON or len(used_features) >= 3:\n",
    "            # Retorna um nó folha (para parar a recursão)\n",
    "            return LeafNode(data, criterion_value=criterion_value, \n",
    "                            _result=class_result, class_name=class_name)\n",
    "\n",
    "        # Seleciona o melhor atributo que não foi usado ainda para dividir os dados\n",
    "        feature_idxs = [i for i in np.arange(data.shape[-1] - 1) if i not in used_features]\n",
    "        selected_feature, _ = self._best_feature(data, feature_idxs)\n",
    "\n",
    "        # Critério de parada: parar se não houver mais atributos para dividir\n",
    "        if selected_feature is None:\n",
    "            return LeafNode(data, criterion_value=criterion_value, \n",
    "                            _result=class_result, class_name=class_name)\n",
    "\n",
    "        # Adiciona o atributo selecionado ao conjunto de atributos usados \n",
    "        # (para evitar a repetição do mesmo atributo)\n",
    "        used_features.add(selected_feature)\n",
    "\n",
    "        # Divide os dados com base no atributo selecionado\n",
    "        left_data = data[data[:, selected_feature] == 0]\n",
    "        right_data = data[data[:, selected_feature] == 1]\n",
    "\n",
    "        # Cria os nós filhos recursivamente\n",
    "        left_node = self._grow(left_data, used_features=used_features)\n",
    "        right_node = self._grow(right_data, used_features=used_features)\n",
    "        \n",
    "        # Retorna um nó de decisão com os nós filhos criados\n",
    "        return Node(left_node, \n",
    "                    right_node,\n",
    "                    data, \n",
    "                    selected_feature, \n",
    "                    feature_name=self.feature_names[selected_feature] if any(self.feature_names) else \"Name not defined\",\n",
    "                    criterion_value=criterion_value,\n",
    "                    _result=class_result, class_name=class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics with K-folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas (Média de 10 folds):\n",
      "{'accuracy': '89.0%', 'precision': '84.0%', 'sensitivity': '94.0%', 'specificity': '84.0%', 'f1_score': '89.0%'}\n"
     ]
    }
   ],
   "source": [
    "# Separando atributos e rótulos\n",
    "X = dataset.iloc[:, :-1].values  # Atributos (todas as colunas exceto a última)\n",
    "y = dataset.iloc[:, -1].values   # Rótulo (última coluna)\n",
    "\n",
    "# Nomes dos atributos e classes\n",
    "feature_names = [\"Empregado\", \"Devedor\", \"Salário acima de 5SM\"]\n",
    "class_names = [\"não\", \"sim\"]\n",
    "\n",
    "# Criando e treinando a árvore de decisão\n",
    "tree = DecisionTree(feature_names=feature_names, class_names=class_names)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Fazendo a validação cruzada com a árvore de decisão usando 10 folds\n",
    "metrics = k_fold_cross_validation(X, y, k_folds=10, decision_tree_class=DecisionTree)\n",
    "\n",
    "#Imprimindo as métricas\n",
    "print(f'Métricas (Média de 10 folds):\\n{metrics}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision-tree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
